{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o186.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\r\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\r\n\tat scala.collection.immutable.List.map(List.scala:293)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\r\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\r\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11396\\1418716892.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mdf_listings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m        \u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistings_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mdf_listing_city\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlisting_city_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20192671\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20192671\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1322\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20192671\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20192671\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o186.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\r\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\r\n\tat scala.collection.immutable.List.map(List.scala:293)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\r\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\r\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"airbnb\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "#  Google Storage File Path\n",
    "listings_file_path = 'gs://assignment2_airbnb/df_listings.csv' \n",
    "listing_city_file_path = 'gs://assignment2_airbnb/df_listing_city.csv' \n",
    "owner_file_path = 'gs://assignment2_airbnb/df_owner.csv' \n",
    "owner_listings_file_path = 'gs://assignment2_airbnb/df_owner_listings.csv' \n",
    "renter_file_path = 'gs://assignment2_airbnb/df_renter.csv' \n",
    "review_file_path = 'gs://assignment2_airbnb/df_review.csv' \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_listings = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(listings_file_path)\n",
    "df_listing_city = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(listing_city_file_path)\n",
    "df_owner = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(owner_file_path)\n",
    "df_owner_listings = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(owner_listings_file_path)\n",
    "df_renter = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(renter_file_path)\n",
    "df_review = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(review_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "#remove the dollar sign from the price column and convert it to float\n",
    "df_listings = df_listings.withColumn(\"price\", regexp_replace(\"price\", \"\\$\", \"\"))\n",
    "df_listings = df_listings.withColumn(\"price\", regexp_replace(\"price\", \"\\,\", \"\"))\n",
    "df_listings = df_listings.withColumn(\"price\", df_listings[\"price\"].cast(\"float\"))\n",
    "#drop the wors where price is zero\n",
    "df_listings = df_listings.filter(df_listings.price > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+--------------------+-----+--------------------+\n",
      "| _c0|                id|                name|price|review_scores_rating|\n",
      "+----+------------------+--------------------+-----+--------------------+\n",
      "|  17|            886695|Small comfortable...| 12.0|                 5.0|\n",
      "|7543|          52998236|EuroParcs Poort v...| 15.0|                 4.4|\n",
      "|7540|          52998638|EuroParcs Poort v...| 15.0|                null|\n",
      "|6396|          53825405|Lovely single bed...| 25.0|                4.67|\n",
      "|8002|           2169229|Enjoy my house ne...| 25.0|                4.96|\n",
      "| 865|          53935725|Cosy room in a su...| 25.0|                 5.0|\n",
      "|8191|          22366418|Private bathroom,...| 27.0|                4.63|\n",
      "|1071|708595196882149607|Betaalbare kamer ...| 28.0|                null|\n",
      "|8583|          49156294|\"Cozy room near  ...| 28.0|                4.86|\n",
      "|8019|           4316692|  Historical Brielle| 28.0|                4.57|\n",
      "|8001|           2139037|Rotterdam, close ...| 29.0|                4.93|\n",
      "| 989|641481947299324543|Cozy room with a ...| 29.0|                 4.3|\n",
      "|8011|           3628774|Beautiful room in...| 30.0|                4.73|\n",
      "|8747|547057809333966852|Charming room/stu...| 30.0|                null|\n",
      "|8543|          45629856|Boutique hostel a...| 30.0|                4.72|\n",
      "|8534|          45481648|Cosy hostel in ci...| 30.0|                4.81|\n",
      "|8171|          20648401|Iconic Cube House...| 30.0|                4.68|\n",
      "|8125|          16506938|Simple room close...| 30.0|                4.87|\n",
      "|8170|          20647352|Iconic Cube House...| 30.0|                4.65|\n",
      "|1025|663370176602699102|Kısa bir süreliği...| 30.0|                null|\n",
      "+----+------------------+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#order the data by price\n",
    "df_listings_ordered = df_listings.orderBy(\"price\", ascending=True)\n",
    "df_listings_ordered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_listings.createOrReplaceTempView(\"listings\")\n",
    "df_listing_city.createOrReplaceTempView(\"listing_city\")\n",
    "df_owner.createOrReplaceTempView(\"owner\")\n",
    "df_owner_listings.createOrReplaceTempView(\"owner_listings\")\n",
    "df_renter.createOrReplaceTempView(\"renter\")\n",
    "df_review.createOrReplaceTempView(\"review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 81 listings with at least 5 listings\n",
      "+---------+----------+------------+\n",
      "| owner_id|avg_rating|num_listings|\n",
      "+---------+----------+------------+\n",
      "|380678517|       5.0|           6|\n",
      "|140775798|      4.96|           6|\n",
      "|  9282300|      4.96|           9|\n",
      "|303405414|       4.9|           7|\n",
      "| 30890942|       4.9|           6|\n",
      "|420783452|      4.88|           6|\n",
      "|  4456680|      4.86|           6|\n",
      "|198405490|      4.85|           7|\n",
      "|430694992|      4.84|           9|\n",
      "|408898089|      4.84|           7|\n",
      "| 89688606|      4.83|           6|\n",
      "|203731852|      4.82|          20|\n",
      "| 46691672|      4.82|           9|\n",
      "|244141635|      4.82|          12|\n",
      "|  5796250|      4.82|          14|\n",
      "| 67005410|       4.8|          17|\n",
      "| 88108496|       4.8|           6|\n",
      "|135487531|      4.79|           7|\n",
      "|177701530|      4.78|          12|\n",
      "|302893992|      4.78|          10|\n",
      "+---------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_nr_listings = 5\n",
    "\n",
    "sql_query_highest_ratings = f\"\"\"SELECT\n",
    "    owner.host_id as owner_id,\n",
    "    round(avg(review_scores_rating), 2) as avg_rating,\n",
    "    count(*) as num_listings\n",
    "FROM\n",
    "    listings, owner, owner_listings\n",
    "WHERE\n",
    "    listings.id = owner_listings.id\n",
    "    and owner.host_id = owner_listings.host_id\n",
    "GROUP BY\n",
    "    owner.host_id\n",
    "HAVING count(*) > {min_nr_listings}\n",
    "ORDER BY\n",
    "    avg_rating desc\"\"\"\n",
    "\n",
    "highest_ratings = spark.sql(sql_query_highest_ratings)\n",
    "\n",
    "print(f\"Found {highest_ratings.count()} listings with at least {min_nr_listings} listings\")\n",
    "\n",
    "highest_ratings.show()\n",
    "highest_ratings.write.format(\"csv\").save(\"gs://assignment2_airbnb/highest_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 owners with listings in at least 2 cities\n",
      "+---------+----------------+----------+\n",
      "|  host_id|       host_name|num_cities|\n",
      "+---------+----------------+----------+\n",
      "|244520390|           Gunni|         2|\n",
      "| 19894111|Arjen & Nathalie|         2|\n",
      "| 10239880|           Lucas|         2|\n",
      "|177701530| Hosted By Wendy|         2|\n",
      "|121985032| Tess - BELVILLA|         2|\n",
      "| 20465009|          Alette|         2|\n",
      "|128826790| Natasja & Mylan|         2|\n",
      "|115324475|             Lin|         2|\n",
      "+---------+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a query to the Spark SQL engine\n",
    "# Check which owners have at least one listing in each of the 3 cities\n",
    "\n",
    "# Table names: listings, listing_city, owner, owner_listings, renter, review\n",
    "# listings(id, name, price, review_scores_rating)\n",
    "# listing_city(id, city)\n",
    "# owner(host_id, host_name)\n",
    "# owner_listings(id, host_id)\n",
    "# renter(renter_id, name)\n",
    "# review(renter_id, listing_id, review)\n",
    "\n",
    "# Take owners that have listings in all of Amsterdam, Rotterdam and Den Haag\n",
    "\n",
    "nr_cities = 2\n",
    "\n",
    "query_owners_in_at_least_2_cities = f\"\"\"\n",
    "SELECT\n",
    "    owner.host_id,\n",
    "    owner.host_name,\n",
    "    count(distinct listing_city.city) as num_cities\n",
    "FROM\n",
    "    owner, owner_listings, listing_city\n",
    "WHERE\n",
    "    owner.host_id = owner_listings.host_id\n",
    "    and owner_listings.id = listing_city.id\n",
    "GROUP BY\n",
    "    owner.host_id, owner.host_name\n",
    "HAVING\n",
    "    count(distinct listing_city.city) >= {nr_cities}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "results_owners_in_alledrie_steden = spark.sql(query_owners_in_at_least_2_cities)\n",
    "\n",
    "print(f\"Found {results_owners_in_alledrie_steden.count()} owners with listings in at least {nr_cities} cities\")\n",
    "results_owners_in_alledrie_steden.show()\n",
    "results_owners_in_alledrie_steden.write.format(\"csv\").save(\"gs://assignment2_airbnb/owners_all_cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 owners with at least 10 listings\n",
      "+---------+---------------+------------+---------+----------+\n",
      "| owner_id|     owner_name|num_listings|avg_price|avg_rating|\n",
      "+---------+---------------+------------+---------+----------+\n",
      "|448782489|        Martijn|          36|   117.97|       4.6|\n",
      "|  5285926|      Diederick|          29|   134.86|       4.5|\n",
      "|  9165668|         Robert|          23|   176.87|       4.6|\n",
      "| 10239880|          Lucas|          22|    104.0|       4.6|\n",
      "| 14574533|Hotel Not Hotel|          21|   189.33|       4.4|\n",
      "|   300966|          Elwin|          20|    226.1|       4.6|\n",
      "|138369331|          Peter|          20|    139.2|       3.9|\n",
      "|203731852|   SWEETS Hotel|          20|   351.45|       4.8|\n",
      "|244520390|          Gunni|          19|   192.05|       4.5|\n",
      "| 67005410|      Feliciano|          17|   149.76|       4.8|\n",
      "|432320567|         Tijmen|          15|    146.4|       4.1|\n",
      "|  5796250|          Remco|          14|   142.79|       4.8|\n",
      "|364305280|         Arnold|          14|   184.29|       4.5|\n",
      "|115403367|          Homey|          14|   116.36|       4.4|\n",
      "|241644101|     Wittenberg|          14|   487.21|       4.5|\n",
      "| 21167882|        Pauline|          13|   318.85|       4.1|\n",
      "|  3040748|           Henk|          13|    91.85|       4.6|\n",
      "|243878598|           Bunk|          12|   238.75|       4.7|\n",
      "|244141635|        Culture|          12|    76.42|       4.8|\n",
      "|177701530|Hosted By Wendy|          12|   211.75|       4.8|\n",
      "+---------+---------------+------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Table names: listings, listing_city, owner, owner_listings, renter, review\n",
    "# listings(id, name, price, review_scores_rating)\n",
    "# listing_city(id, city)\n",
    "# owner(host_id, host_name)\n",
    "# owner_listings(id, host_id)\n",
    "# renter(renter_id, name)\n",
    "# review(renter_id, listing_id, review)\n",
    "\n",
    "min_listings = 10\n",
    "\n",
    "# Find owners with at least 10 listings\n",
    "query_owners_with_at_least_10_listings = \"\"\"\n",
    "SELECT\n",
    "    owner.host_id as owner_id,\n",
    "    owner.host_name as owner_name,\n",
    "    COUNT(*) as num_listings,\n",
    "    ROUND(AVG(listings.price), 2) as avg_price,\n",
    "    ROUND(AVG(listings.review_scores_rating), 1) as avg_rating\n",
    "FROM\n",
    "    listings, owner, owner_listings\n",
    "WHERE\n",
    "    listings.id = owner_listings.id\n",
    "    and owner.host_id = owner_listings.host_id\n",
    "GROUP BY\n",
    "    owner.host_id, owner.host_name\n",
    "HAVING COUNT(*) > 10\n",
    "ORDER BY\n",
    "    num_listings DESC\n",
    "\"\"\"\n",
    "\n",
    "results_owners_10_listings = spark.sql(query_owners_with_at_least_10_listings)\n",
    "print(f\"Found {results_owners_10_listings.count()} owners with at least {min_listings} listings\")\n",
    "results_owners_10_listings.show(20)\n",
    "results_owners_10_listings.write.format(\"csv\").save(\"gs://assignment2_airbnb/owners_10_listings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|avg_price|avg_rating|\n",
      "+---------+----------+\n",
      "|   195.62|       4.8|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate average price and rating over all listings\n",
    "query_average_price_rating = \"\"\"\n",
    "SELECT\n",
    "    ROUND(AVG(listings.price), 2) as avg_price,\n",
    "    ROUND(AVG(listings.review_scores_rating), 1) as avg_rating\n",
    "FROM\n",
    "    listings\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "avg_price_rating=spark.sql(query_average_price_rating).show(20)\n",
    "avg_price_rating.write.format(\"csv\").save(\"gs://assignment2_airbnb/avg_price_rating.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d0b357807bdd6cd8aa1b36cc9830dd9025da982ad3117fc0608eb1481b3304b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
